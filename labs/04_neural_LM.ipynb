{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Neural Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "\n",
    "- Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/))\n",
    "- [Chapter 6: Vector Semantics and Embeddings](https://web.stanford.edu/~jurafsky/slp3/6.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brownfortress/NLU-2024-labs/blob/main/labs/04_neural_LM.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "#### Word Embeddings\n",
    "- One-hot encoding\n",
    "- TF-IDF\n",
    "- Pointwise mutual information \n",
    "\n",
    "#### Word embedding similarity\n",
    "- Cosine distance\n",
    "\n",
    "#### Language modelling with Neural Networks\n",
    "\n",
    "#### Neural Networks in Pytorch\n",
    "- Embeddings\n",
    "- Recurrent Neural Network (RNN)\n",
    "\n",
    "#### Train and Test a Neural Network\n",
    "- Optimizer\n",
    "- Loss function\n",
    "- Iteration over batches\n",
    "\n",
    "# References\n",
    "- RNN: https://d2l.ai/chapter_recurrent-neural-networks/index.html \n",
    "- LSTM: https://d2l.ai/chapter_recurrent-modern/lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Words as Vectors (Embeddings)\n",
    "In natural language processing (NLP), [**word embedding**](https://en.wikipedia.org/wiki/Word_embedding) is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from space with many dimensions per word to a continuous vector space with a much lower dimension.\n",
    "<br>\n",
    "- Computing word embeddings is the process by which words are transformed into vectors of (real) numbers.\n",
    "- Definition of meaning by distributional similarity/usage: similar words are close in \"space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. One-Hot Encoding\n",
    "- sparse vectors\n",
    "- most basic way to turn a token into a vector\n",
    "- method\n",
    "    - associate a unique integer index with every word in a vocabulary of size $V$\n",
    "    - turn this integer index $i$ into a binary vector of size $V$ (i.e. the size of the vocabulary)\n",
    "    - the vector has all values `0` except for the $i$ th entry, which is `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-Occurence Matrices and Word as Vectors\n",
    "### 2.1. Term-Document Matrix\n",
    "- could be used to represent words, where dimension are documents\n",
    "### 2.2. TF-IDF\n",
    "- sparse vectors\n",
    "- generally used to represent documents, where dimensions are words\n",
    "\n",
    "#### TF: Term Frequency\n",
    "$$\\text{tf}_{t,d} = \\text{count}(t,d)$$\n",
    "$$\\text{tf}_{t,d} = \\log_{10}(\\text{count}(t,d) + 1)$$\n",
    "\n",
    "`+1` is because log of 0 is undefined.\n",
    "\n",
    "Alternatively:\n",
    "\n",
    "$$\\text{tf}_{t,d} = \n",
    "\\begin{cases}\n",
    "1 + \\log_{10}(\\text{count}(t,d)), & \\text{if count}(t,d) > 0\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### IDF: Inverse Document Frequency\n",
    "\n",
    "$$\\text{idf}_t = \\frac{N}{\\text{df}_t}$$\n",
    "\n",
    "Usually in log space, like term frequency.\n",
    "\n",
    "$$\\text{idf}_t = \\log_{10}(\\frac{N}{\\text{df}_t})$$\n",
    "\n",
    "- $\\text{df}_t$ is the number of documents in which term $t$ occurs.\n",
    "- $N$ is the total number of documents in the collection.\n",
    "\n",
    "The __tf-idf__ weighted value $w_{t,d}$ for word $t$ in document $d$ is the combination of $\\text{tf}_{t,d}$ and $\\text{idf}_t$:\n",
    "\n",
    "$$w_{t,d} = \\text{tf}_{t,d} \\times \\text{idf}_t$$\n",
    "\n",
    "### 2.3. Term-Term Matrix\n",
    "- a.k.a. \"word-word\" or \"word-context\" matrix\n",
    "- words are represented by a function of the counts of nearby words \n",
    "- size $|V| \\times |V|$, where $V$ is the vocabulary size\n",
    "    - usually context is taken to be a document or words in a window around the target word\n",
    "\n",
    "### 2.4. Pointwise Mutual Information (PMI) and Positive Pointwise Mutual Information (PPMI)\n",
    "- used for term-term matrices\n",
    "- the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.\n",
    "\n",
    "#### 2.4.1. Pointwise Mutual Information (PMI)\n",
    "- a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:\n",
    "\n",
    "$$I(x, y) = \\log_2 \\frac{P(x, y)}{P(x)P(y)}$$\n",
    "\n",
    "\n",
    "The pointwise mutual information between a target word $w$ and a context word $c$ is defined as:\n",
    "\n",
    "$$\\text{PMI}(w, c) = \\log_2 \\frac{P(w, c)}{P(w)P(c)}$$\n",
    "\n",
    "#### 2.4.2. Positive Pointwise Mutual Information (PMI)\n",
    "- PMI values range from negative to positive infinity.\n",
    "- negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable\n",
    "- it is more common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero\n",
    "\n",
    "$$\\text{PPMI}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P(c)}, 0)$$\n",
    "\n",
    "#### 2.4.3. PPMI Matrix\n",
    "To get a PPMI matrix from a co-occurrence matrix $F$, where $W$ rows are words and $C$ columns are contexts, and $f_{ij}$ is the number of times word $w_i$ appears in context $c_j$ (i.e. value of the cell).\n",
    "\n",
    "$$P(w,c) = \\frac{f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(w) = \\frac{\\sum_{j=1}^C f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "$$P(c) = \\frac{\\sum_{i=1}^W f_{ij}}{\\sum_{i=1}^W \\sum_{j=1}^C f_{ij}}$$\n",
    "\n",
    "- PMI has the problem of being biased toward infrequent events: very rare words tend to have very high PMI values.\n",
    "- Thus, $P(c)$ is computed as $P_{\\alpha}(c)$ that raises the probability of the context word to the power of $\\alpha$ (e.g. $0.75$)\n",
    "    - Alternative is Laplace smoothing\n",
    "\n",
    "$$\\text{PPMI}_{\\alpha}(w, c) = \\max(\\log_2 \\frac{P(w, c)}{P(w)P_{\\alpha}(c)}, 0)$$\n",
    "\n",
    "$$P_{\\alpha}(c) = \\frac{\\text{count}(c)^{\\alpha}}{\\sum_{c}\\text{count}(c)^{\\alpha}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vector Similarity\n",
    "- Two words are similar in meaning if their context __vectors__ are similar;\n",
    "- __Cosine similarity__ measures the similarity between two vectors of an __inner product space__. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction.\n",
    "\n",
    "### 3.1. Dot Product\n",
    "\n",
    "- dot product (inner product)\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = \\sum^N_{i=1}v_i w_i = v_1 w_1 + v_2 w_2 + ... + v_N w_N$$\n",
    "\n",
    "- vector length (L2 norm $||v||_2$)\n",
    "\n",
    "$$|\\vec{v}| = \\sqrt{\\sum^N_{i=1} v_i^2}$$ \n",
    "\n",
    "$$ |\\vec{v}| = \\sqrt{\\vec{v}\\cdot\\vec{v}} = \\sqrt{\\sum^N_{i=1} v_i v_i} = \\sqrt{\\sum^N_{i=1} v_1 v_1 + v_2 v_2 + ... + v_N v_N}$$\n",
    "\n",
    "### 3.2. Cosine Similarity\n",
    "\n",
    "- L2 normalized dot product of 2 vectors\n",
    "    - $\\theta$ is the angle between $\\vec{v}$ and $\\vec{w}$\n",
    "\n",
    "$$\\vec{v}\\cdot\\vec{w} = |\\vec{v}||\\vec{w}|\\cos\\theta$$\n",
    "\n",
    "$$\\cos\\theta = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|}$$\n",
    "\n",
    "$$\\text{CosSim}(\\vec{v},\\vec{w}) = \\frac{\\vec{v}\\cdot\\vec{w}}{|\\vec{v}||\\vec{w}|} = \\frac{\\sum^N_{i=1}v_i w_i}{\\sqrt{\\sum^N_{i=1} v_i^2} \\sqrt{\\sum^N_{i=1} w_i^2}}$$\n",
    "\n",
    "#### Cosine Distance\n",
    "$$\\text{Cosine Distance}(\\vec{v}, \\vec{w}) = 1 - \\text{Cosine Similarity}(\\vec{v}, \\vec{w})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you are on Colab\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "txt = 'metropolis'\n",
    "doc = nlp(txt)\n",
    "\n",
    "tok = doc[0]  # let's take Rome\n",
    "\n",
    "print(\"string:\", tok.text)\n",
    "print(\"vector dimension:\", len(tok.vector))\n",
    "print(\"spacy vector norm:\", tok.vector_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# let's get Paris & compare its vector to rome\n",
    "paris = nlp('city')[0]\n",
    "print(paris.text)\n",
    "\n",
    "print(\"spacy CosSim({}, {}):\".format(tok.text, paris.text), tok.similarity(paris))\n",
    "print(\"scipy CosSim({}, {}):\".format(tok.text, paris.text), 1 - cosine(tok.vector, paris.vector))\n",
    "\n",
    "tok2 = nlp('computer')[0]\n",
    "print(tok2.text)\n",
    "print(\"spacy CosSim({}, {}):\".format(tok.text, tok2.text), tok.similarity(tok2))\n",
    "print(\"scipy CosSim({}, {}):\".format(tok.text, tok2.text), 1 - cosine(tok.vector, tok2.vector))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to introduce the concept of word embedding to a friend, [Semantle](https://semantle.com) game is a good starting point. It is based on word2vec model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Language Models with Neural Networks\n",
    "\n",
    "While we have already seen a language model based on n-grams, in this lab we are going to develop a language model using a neural architecture. Neural LM can be used to compute word embeddings.\n",
    "\n",
    "## 4.1 Task definition\n",
    "\n",
    "To model the probability distribution over a sequence, we are going to use the Chain Rule as we have seen in LAB 3:\n",
    "\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "However, at that time we used ngram to truncate the previous context ($N-1$), to compute meaningful probabilities. While using neural models, we will let the model decide by itself how to manage the previous context and thus which are the tokens relevant for the prediction.\n",
    "\n",
    "\n",
    "## 4.2 Recurrent Neural Networks (RNN)\n",
    "\n",
    "One of the most suitable neural architectures for the Language Model task is the Recurrent Neural Network (RNN). The architecture is composed of an RNN layer (vanilla, LSTM, GRU) and a linear+softmax layer that outputs the probability over the dictionary. Indeed, the size of the output vector is equal to the size of the dictionary, i.e. the model cannot predict tokens that are not present in the vocabulary. <br>\n",
    "\n",
    "> LM task in RNN can be tackled as a sequence labelling task (each input token has an output label) in which the input sequence is $ input = \\{w_1, w_2, w_{n-1}\\}$ and the output is $ output = \\{w_2, w_3, w_{n}\\}$\n",
    "\n",
    "\n",
    "\n",
    "***Example***:\n",
    " > For the input sentence ***\"I go to Miami\"***, the input sequence of the model is ***\"I go to\"*** and the target/output sequence is ***\"go to Miami\"***.\n",
    "\n",
    "\n",
    "\n",
    "***Notice***:\n",
    "\n",
    "> - To properly model the sequence probabilities we need to add boundary markers \\<s\\> and \\</s\\>.\n",
    "\n",
    "> - However, in LM RNN only the end of sentence token \\</s\\> is usually used unless we need \\<s\\> for some reason.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://i.postimg.cc/zGH99MFY/rnn-lm.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n",
    "In the image below you can see a working example of a language model with RNN. \n",
    "<p align=\"center\">\n",
    "    <img src=\"https://i.postimg.cc/fydQNrYP/LM-RNN.png\" alt=\"drawing\" width=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we define the architecture of our model using PyTorch. In the `__init__` method, we define the class of our model and we instantiate all the layers that we are going to use. In the `forward` method we define the interactions among the instantiated layers, in other words, we design the architecture of the model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# RNN Elman version\n",
    "# We are not going to use this since for efficiently purposes it's better to use the RNN layer provided by pytorch  \n",
    "\n",
    "class RNN_cell(nn.Module):\n",
    "    def __init__(self,  hidden_size, input_size, output_size, vocab_size, dropout=0.1):\n",
    "        super(RNN_cell, self).__init__()\n",
    "        \n",
    "        self.W = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.U = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, prev_hidden, word):\n",
    "        input_emb = self.W(word)\n",
    "        prev_hidden_rep = self.U(prev_hidden)\n",
    "        # ht = σ(Wx + Uht-1 + b)\n",
    "        hidden_state = self.sigmoid(input_emb + prev_hidden_rep)\n",
    "        # yt = σ(Vht + b)\n",
    "        output = self.output(hidden_state)\n",
    "        return hidden_state, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_RNN(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_RNN, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.rnn = nn.RNN(emb_size, hidden_size, n_layers, bidirectional=False, batch_first=True)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        rnn_out, _  = self.rnn(emb)\n",
    "        output = self.output(rnn_out).permute(0,2,1)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Data loading \n",
    "\n",
    "We are going to see this part in details in the next lab. Anyhow, let's have an overview. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0' # it can be changed with 'cpu' if you do not have a gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the corpus \n",
    "\n",
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            output.append(line.strip() + \" \" + eos_token)\n",
    "    return output\n",
    "\n",
    "# Vocab with tokens to ids\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Colab, run these lines\n",
    "# !wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.test.txt\n",
    "# !wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.valid.txt\n",
    "# !wget -P dataset/PennTreeBank https://raw.githubusercontent.com/BrownFortress/NLU-2024-Labs/main/labs/dataset/PennTreeBank/ptb.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_raw = read_file(\"dataset/PennTreeBank/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/PennTreeBank/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/PennTreeBank/ptb.test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab is computed only on training set \n",
    "# We add two special tokens end of sentence and padding \n",
    "vocab = get_vocab(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class computes and stores our vocab \n",
    "# Word to ids and ids to word\n",
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences of tokens to corresponding computed in Lang class\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    \n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(DEVICE)\n",
    "    new_item[\"target\"] = target.to(DEVICE)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "# You can reduce the batch_size if the GPU memory is not enough\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Train and validate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            loss = eval_criterion(output, sample['target'])\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes \n",
    "# A large model tends to overfit\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "# Don't forget to experiment with a lower training batch size\n",
    "# Increasing the back propagation steps can be seen as a regularization step\n",
    "\n",
    "# With SGD try with an higher learning rate (> 1 for instance)\n",
    "lr = 0.0001 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(DEVICE)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = None\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "#If the PPL is too high try to change the learning rate\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "print('Test ppl: ', final_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model makes you happy and you want to reuse it, you have [to save it and load it](https://pytorch.org/tutorials/beginner/saving_loading_models.html). \n",
    "In PyTorch this is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model\n",
    "# path = 'model_bin/model_name.pt'\n",
    "# torch.save(model.state_dict(), path)\n",
    "# To load the model you need to initialize it\n",
    "# model = LM_RNN(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "# Then you load it\n",
    "# model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Exam Exercise\n",
    "## Part 1 (4 points)\n",
    "In this, you have to modify the baseline LM_RNN by adding a set of techniques that might improve the performance. In this, you have to add one modification at a time incrementally. If adding a modification decreases the performance, you can remove it and move forward with the others. However, in the report, you have to provide and comment on this unsuccessful experiment.  For each of your experiments, you have to print the performance expressed with Perplexity (PPL).\n",
    "<br>\n",
    "One of the important tasks of training a neural network is  hyperparameter optimization. Thus, you have to play with the hyperparameters to minimise the PPL and thus print the results achieved with the best configuration (in particular <b>the learning rate</b>). \n",
    "These are two links to the state-of-the-art papers which use vanilla RNN [paper1](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5947611), [paper2](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf). \n",
    "\n",
    "**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***).\n",
    "\n",
    "1. Replace RNN with a Long-Short Term Memory (LSTM) network --> [link](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "2. Add two dropout layers: --> [link](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)\n",
    "    - one after the embedding layer, \n",
    "    - one before the last linear layer\n",
    "3. Replace SGD with AdamW --> [link](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (11 points)\n",
    "**Mandatory requirements**: For the following experiments the perplexity must be below 250 (***PPL < 250***) and it should be lower than the one achieved in Part 1.1 (i.e. base LSTM).\n",
    "\n",
    "Starting from the `LM_RNN` in which you replaced the RNN with a LSTM model, apply the following regularisation techniques:\n",
    "- Weight Tying \n",
    "- Variational Dropout (no DropConnect)\n",
    "- Non-monotonically Triggered AvSGD \n",
    "\n",
    "These techniques are described in [this paper](https://openreview.net/pdf?id=SyyGPP0TZ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
