{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Experimental Methodology in Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding \n",
    "    - the role and types of evaluation in NLP/ML\n",
    "    - the lower and upper bounds of performance\n",
    "    - correct usage of data for experimentation\n",
    "    - evaluation metrics\n",
    "    \n",
    "- Learning how to use `scikit-learn` to perform a text classification experiment\n",
    "    - provided baselines\n",
    "    - text vectorization\n",
    "    - evaluation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!-- ### Requirements \n",
    "- [scikit-learn](https://scikit-learn.org/)\n",
    "    - run `pip install scikit-learn`\n",
    "- [matplotlib](https://matplotlib.org/)\n",
    "    - run `pip install matplotlib`  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Basic Concepts of Experimental Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Lower & Upper Bounds of the Performance\n",
    "\n",
    "#### Lower Bound: Baseline\n",
    "Trivial solution to the problem: \n",
    "\n",
    "- _random_: random decision\n",
    "- _chance_: random decision w.r.t. the distribution of categories in the training data\n",
    "- _majority_: assign everything to the largest category etc.\n",
    "- or the *state-of-the-art* model that you want to beat\n",
    "\n",
    "#### Upper Bound: Inter-rater agreement\n",
    "Usually human performance. Human performance is evaluated with inter-rater agreement.\n",
    "\n",
    "A system is expected to perform within the lower and upper bounds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1. Training-Testing Split\n",
    "\n",
    "Often Data Set is split into the following parts:\n",
    "\n",
    "- _Training_: for training / extracting rules / etc.\n",
    "- _Development_ (Validation, or simply Dev): for optimization / intermediate evaluation\n",
    "- _Testing_: for the final evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.2.1. [K-Fold Cross-Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into $k$ equal sized subsamples. Of the $k$ subsamples, a single subsample is retained as the validation data for testing the model, and the remaining $k − 1$ subsamples are used as training data. The cross-validation process is then repeated $k$ times, with each of the $k$ subsamples used exactly once as the validation data. The $k$ results can then be averaged to produce a single estimation.\n",
    "\n",
    "\\*\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png \" alt=\"kfold\" width=\"500\"/>\n",
    "\n",
    "\\*Image from https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "- Random K-Fold Cross-Validation splits data into $K$ equal folds\n",
    "- Stratified K-Fold Cross-Validation additionally makes sure that the distribution of target labels is similar across different folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The general procedure is as follows:\n",
    "\n",
    "- Shuffle the dataset randomly\n",
    "- Split the dataset into $k$ folds\n",
    "- For each unique group:\n",
    "    - Take the group as a hold out or test data set\n",
    "    - Take the remaining groups as a training data set\n",
    "    - Fit a model on the training set and evaluate it on the test set\n",
    "    - Retain the evaluation score and discard the model\n",
    "- Summarize the model performance averaging the evaluation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Contingency Table\n",
    "\n",
    "A [contingency table](https://en.wikipedia.org/wiki/Contingency_table) (also known as a _cross tabulation_ or _crosstab_) is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables. For the binary classification into positive (_POS_) and negative (_NEG_) classes, the predictions of a model (_HYP_, for hypotheses) with respect to the true labels (_REF_, for references) can be represented as the  matrix.\n",
    "\n",
    "|     |         | REF     |         |\n",
    "|-----|---------|:-------:|:-------:|\n",
    "|     |         | __POS__ | __NEG__ |\n",
    "| HYP | __POS__ | TP      | FP      |\n",
    "|     | __NEG__ | FN      | TN      |\n",
    "\n",
    "\n",
    "Where:\n",
    "- __TP__: True Positives (usually denoted as $a$)\n",
    "- __FP__: False Positive ($b$)\n",
    "- __FN__: False Negatives ($c$)\n",
    "- __TN__: True Negative ($d$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. The Simplest Case: Accuracy\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Num. of Correct Decisions}}{\\text{Total Num. of Instances}} $$\n",
    "\n",
    "- Known number of instances\n",
    "- Single decision for each instance \n",
    "- Single correct answer for each instance \n",
    "- All errors are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TN}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__What if TN is infinite or unknown?__\n",
    "\n",
    "e.g.: Number of irrelevant queries to a search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Precision & Recall\n",
    "\n",
    "|     |         | REF     |         |             |\n",
    "|-----|---------|:-------:|:-------:|-------------|\n",
    "|     |         | __POS__ | __NEG__ |             |\n",
    "| HYP | __POS__ | TP      | FP      | _Precision_ |\n",
    "|     | __NEG__ | FN      | TN      |             |\n",
    "|     |         | _Recall_ |        |             |\n",
    "\n",
    "\n",
    "$$ \\text{Precison} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/878/1*Ub0nZTXYT8MxLzrz0P7jPA.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "__2 Values__: \n",
    "\n",
    "Precision-Recall Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. F-Measure\n",
    "\n",
    "- Harmonic Mean of Precision & Recall \n",
    "- Usually evenly weighted\n",
    "\n",
    "\n",
    "$$F_{\\beta} = \\frac{(1 + \\beta^2) ∗ \\text{Precision} ∗ \\text{Recall}}{\\beta^2 ∗ \\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "Most common value of $\\beta = 1$\n",
    "\n",
    "$ F_1 = \\frac{2 ∗ \\text{Precision} ∗ \\text{Recall}}{\\text{Precision} + \\text{Recall}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.4. Micro, Macro and (Macro-) Weighted Averaging\n",
    "\n",
    "In a Multi-Class setting per-class scores are averaged to produce a single score.\n",
    "There are several ways the scores could be averaged. \n",
    "\n",
    "__Micro Averaging__\n",
    "\n",
    "We compute scores summing over True Positive, True Negative, False Positive and False Negatives.\n",
    "\n",
    "__Macro Averaging__\n",
    "\n",
    "We first compute scores per class, then average the scores ignoring their distribution in the test set.\n",
    "\n",
    "__(Macro-) Weighted Averaging__\n",
    "\n",
    "Similar to Macro Averaging, but we additionally weight the scores by the class-frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Precision Example\n",
    "\n",
    "Let's assume we have 3 classes. The precision formula from above is:\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}$$\n",
    "\n",
    "$$\\text{Micro Precision} = \\frac{\\text{TP}_1 + \\text{TP}_2 +\\text{TP}_3}{(\\text{TP}_1 + \\text{TP}_2 +\\text{TP}_3)+(\\text{FP}_1 + \\text{FP}_2 +\\text{FP}_3)}$$\n",
    "\n",
    "$$\\text{Macro Precision} = \\frac{P_1 + P_2 + P_3}{3} = P_1 * \\frac{1}{3} + P_2 * \\frac{1}{3} + P_3 * \\frac{1}{3}$$\n",
    "\n",
    "$$\\text{Weighted Precision} = P_1 * \\frac{S_1}{N} + P_2 * \\frac{S_2}{N} + P_3 * \\frac{S_3}{N}$$\n",
    "\n",
    "Where:\n",
    "- $S$ is the support for the class (i.e. number of observations with that labels)\n",
    "- $N$ is the total number of observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Classification with Scikit-Learn\n",
    "\n",
    "- Loading Data\n",
    "- Baselines\n",
    "- Training Classifier\n",
    "- Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Loading and Inspecting a Dataset\n",
    "\n",
    "`scikit-learn` comes with several toy datasets.\n",
    "Let's use one of those (iris) to perform a simple classification experiment.\n",
    "\n",
    "Iris dataset: https://archive.ics.uci.edu/ml/datasets/iris \n",
    " - 3 linearly and not-linearly separable classes\n",
    "    \n",
    "The iris dataset is a classic and very easy multi-class classification dataset.\n",
    "\n",
    "| Property          | Value |\n",
    "|-------------------|-------|\n",
    "| Classes           |   3 |\n",
    "| Samples per class |  50 |\n",
    "| Samples total     | 150 |\n",
    "| Dimensionality    |   4 | \n",
    "| Features          | real, positive | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from collections import Counter\n",
    "data = load_iris()\n",
    "\n",
    "print(\"Classes: {}\".format(len(list(data.target_names))))\n",
    "print(\"Samples: {}\".format(len(data.data)))\n",
    "print(\"Dimensionality: {}\".format(len(list(data.feature_names))))\n",
    "print(\"Samples per Class: {}\".format(dict(Counter(list(data.target)))))\n",
    "\n",
    "print(data.data[0])  # a feature vector\n",
    " \n",
    "print(data.data.shape)  # the  matrix shape for data\n",
    "print(data.target.shape)  # the  matrix shape for labels\n",
    "\n",
    "# print(data.DESCR)  # full dataset description\n",
    "# print(data.data)  # all the features\n",
    "# print(data.target) # all the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. Splitting the Dataset\n",
    "\n",
    "- Random K-Fold Split\n",
    "- Stratified K-Fold Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "n_split = 5\n",
    "random_split = KFold(n_splits= n_split, shuffle=True)\n",
    "# Random and Stratified splits return the indexes of the data points (X or Y)  \n",
    "# and not the data points themselves\n",
    "\n",
    "for train_index, test_index in random_split.split(data.data):\n",
    "    \n",
    "    print(\"Samples per Class in Training: {}\".format(dict(Counter(list(data.target[train_index])))))\n",
    "    print(\"Samples per Class in Testing: {}\".format(dict(Counter(list(data.target[test_index])))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_bars(values, labels, width=0.35, title=\"\"):\n",
    "    x = np.arange(len(labels)) \n",
    "    _, ax = plt.subplots(figsize=(12,5))\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Classes')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x, labels)\n",
    "    \n",
    "    center = round(len(values)/2)\n",
    "    for id_x, temp in enumerate(values):\n",
    "        new_x = x + width/len(values) * (id_x-center)\n",
    "        lab = 'split'+str(id_x+1)\n",
    "        ax.bar(new_x, temp, width/len(values), label=lab)\n",
    "\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "split_train = []\n",
    "split_test = []\n",
    "\n",
    "for train_index, test_index in random_split.split(data.data):    \n",
    "    split_train.append([ v for _, v in sorted(Counter(list(data.target[train_index])).items())])\n",
    "    split_test.append([ v for _, v in sorted(Counter(list(data.target[test_index])).items())])\n",
    "\n",
    "\n",
    "plot_bars(split_train, [0,1,3], title='Random split Train')\n",
    "\n",
    "plot_bars(split_test, [0,1,3], title='Random split Test')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "stratified_split = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
    "    \n",
    "    print(\"Samples per Class in Training: {}\".format(dict(Counter(list(data.target[train_index])))))\n",
    "    print(\"Samples per Class in Testing: {}\".format(dict(Counter(list(data.target[test_index])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train = []\n",
    "split_test = []\n",
    "for train_index, test_index in stratified_split.split(data.data, data.target):    \n",
    "    split_train.append([ v for _, v in sorted(Counter(list(data.target[train_index])).items())])\n",
    "    split_test.append([ v for _, v in sorted(Counter(list(data.target[test_index])).items())])\n",
    "    \n",
    "plot_bars(split_train, [0,1,3], title='Stratified split Train')\n",
    "plot_bars(split_test, [0,1,3], title='Stratified split Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3. Training and Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.1. Classification Process\n",
    "\n",
    "- Select the classification algorithm from [Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html)\n",
    "- Train on training data\n",
    "- Predict labels on testing data\n",
    "- Score prediction comparing predicted and reference labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# choose classification algorithm & initialize it\n",
    "clf = GaussianNB()\n",
    "\n",
    "# for each training/testing fold\n",
    "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
    "    # train (fit) model\n",
    "    clf.fit(data.data[train_index], data.target[train_index])\n",
    "    # predict test labels\n",
    "    clf.predict(data.data[test_index])\n",
    "    # score the model (using average accuracy for now)\n",
    "    accuracy = clf.score(data.data[test_index], data.target[test_index])\n",
    "    print(\"Accuracy: {:.3}\".format(accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.2. Baselines\n",
    "\n",
    "Scikit-learn provides baselines via `DummyClassifier` class that takes `strategy` argument. The following baselines can be obtaining:\n",
    "\n",
    "- random baseline: `uniform`\n",
    "- chance baseline: `stratified`\n",
    "- majority baseline: `most_frequent`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "random_clf = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
    "    random_clf.fit(data.data[train_index], data.target[train_index])\n",
    "    random_clf.predict(data.data[test_index])\n",
    "    accuracy = random_clf.score(data.data[test_index], data.target[test_index])\n",
    "    \n",
    "    print(\"Accuracy: {:.3}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Try `stratified` and `most_frequent` strategies and observe how these affect the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = stratified or most_frequent\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy= X)\n",
    "# Random and Stratified splits return the indexes of the data points (X or Y)  \n",
    "# and not the data point itself\n",
    "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
    "    dummy_clf.fit(data.data[train_index], data.target[train_index])\n",
    "    dummy_clf.predict(data.data[test_index])\n",
    "    accuracy = dummy_clf.score(data.data[test_index], data.target[test_index])\n",
    "    \n",
    "    print(\"Accuracy: {:.3}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.3. Better Classification Report\n",
    "\n",
    "scikit-learn provides functions to report more informative performance values using [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# choose classification algorithm & initialize it\n",
    "clf = GaussianNB()\n",
    "\n",
    "# for each training/testing fold\n",
    "for train_index, test_index in stratified_split.split(data.data, data.target):\n",
    "    # train (fit) model\n",
    "    clf.fit(data.data[train_index], data.target[train_index])\n",
    "    # predict test labels\n",
    "    hyps = clf.predict(data.data[test_index])\n",
    "    refs = data.target[test_index]\n",
    "    \n",
    "    report = classification_report(refs, hyps, target_names=data.target_names)\n",
    "    \n",
    "    print(report)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 3.3.4. Cross-Validation Evaluation\n",
    "\n",
    "The cross-validation procedure and function of scikit-learn are described in [the documentation](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# choose classification algorithm & initialize it\n",
    "clf = GaussianNB()\n",
    "# get scores\n",
    "scores = cross_val_score(clf, data.data, data.target, cv=5)\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cross-Validation using custom split and scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# choose classification algorithm & initialize it\n",
    "clf = GaussianNB()\n",
    "# scoring providing our custom split & scoring using \n",
    "scores = cross_validate(clf, data.data, data.target, cv=stratified_split, scoring=['f1_macro'])\n",
    "\n",
    "print(sum(scores['test_f1_macro'])/len(scores['test_f1_macro']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "- Read the [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- Experiment with different evaluation scores\n",
    "    - For instance, change f1_macro with f1_micro or f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "scores = cross_validate(clf, data.data, data.target, cv=stratified_split, scoring=['TO REPLACE'])\n",
    "\n",
    "print(sum(scores['test_TO REPLACE'])/len(scores['test_TO REPLACE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4 Vectorizing Text\n",
    "\n",
    "> The raw data, i.e. sequences of symbols with variable lengths, cannot be fed directly into the algorithms themselves as most of them expect numerical feature vectors with a fixed size.\n",
    "\n",
    "Consequently, the additional step that **text classification** requires is vectorization. Vectorization converts text into a vector of numerical values. `scikit-learn` provides several vectorization methods in `sklearn.feature_extraction` [module](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). Most commonly used ones are:\n",
    "\n",
    "- Count Vectorization\n",
    "- TF-IDF Vectorization\n",
    "\n",
    "Problem:\n",
    "-  We need to covert the string e.g. 'Hello World' into a matrix in which the rows are the vectors representing the words. The the word-vector dimensionality, i.e. the number of columns, depends on the encoding method.\n",
    "   \n",
    "Result: \n",
    "```\n",
    "      [\n",
    " Hello [0, 1],\n",
    " World [1, 0],\n",
    "      ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Bag-of-Words Representation\n",
    "\n",
    "[Count Vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) implements the following vectorization procedure. \n",
    "\n",
    "- *tokenizing* strings, i.e. splitting a string into tokens using for instance white-spaces and punctuation as token separators. Then, for each token it gives integer id. \n",
    "\n",
    "- *counting* the occurrences of tokens in each document.\n",
    "\n",
    "- *normalizing* and *weighting* with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "\n",
    "Each token is considered to be a __feature__ and the vector of all the token frequencies for a given document is considered a multivariate __sample__. Consequently, a corpus of documents is represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "> Notice: If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. *(See LAB 1 for  vocab calculation)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) implements both tokenization and occurrence counting in a single class, and it is possible to provide many parameters. \n",
    "\n",
    "It can take an external preprocessor or perform the following preprocessing steps (read documentation for details):\n",
    "\n",
    "- __strip_accents__: remove accents and perform other character normalization during the preprocessing step.\n",
    "- __lowercase__: convert all characters to lowercase before tokenizing.\n",
    "- __stop_words__: apply a built-in stop word list for English is used. \n",
    "- __token_pattern__: regular expression denoting what constitutes a *token* for tokenization\n",
    "- __ngram_range__: The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. (We will see ngrams the next lab)\n",
    "- __max_df__: maximum frequency cut-off: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). \n",
    "- __min_df__: minimum frequency cut-off: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. \n",
    "- __vocabulary__: externally provided vocabulary\n",
    "- __binary__: If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. [TF-IDF Vectorization](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)  \n",
    "TF-IDF Vectorization = Count Vectorization + TF-IDF Transformation\n",
    "\n",
    "> Transforms a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "> __Tf__ means term-frequency while __tf-idf__ means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.\n",
    "\n",
    "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "$$tf_{t,d} = \\frac{c_{t,d}}{|d|} $$\n",
    "where $c$ is the count of term $t$ in the document $d$. While, $|d|$ is the total number of terms in the document.\n",
    "$$idf(t, D) = log\\frac{N}{|\\{d \\in D : t \\in D \\}|}$$\n",
    "\n",
    "where N is the number of document in the corpus i.e. N = |D|. $|\\{d \\in D : t \\in D \\}|$ is the number of documents in which the term $t$ appears.\n",
    "\n",
    "$$tf{-}idf(t,d,D) = tf(t, d) \\cdot{idf(t,D)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3. Vectorization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1]\n",
      " [0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 2 0 1 1]\n",
      " [0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 3 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    'who plays luke on star wars new hope',\n",
    "    'show credits for the godfather',\n",
    "    'who was the main actor in the exorcist',\n",
    "    'find the female actress from the movie she \\'s the man',\n",
    "    'who played dory on finding nemo'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# use fit_transform to 'learn' the features and vectorize the data\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectors.toarray())  # print numpy vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>credits</th>\n",
       "      <th>dory</th>\n",
       "      <th>exorcist</th>\n",
       "      <th>female</th>\n",
       "      <th>find</th>\n",
       "      <th>finding</th>\n",
       "      <th>for</th>\n",
       "      <th>from</th>\n",
       "      <th>...</th>\n",
       "      <th>on</th>\n",
       "      <th>played</th>\n",
       "      <th>plays</th>\n",
       "      <th>she</th>\n",
       "      <th>show</th>\n",
       "      <th>star</th>\n",
       "      <th>the</th>\n",
       "      <th>wars</th>\n",
       "      <th>was</th>\n",
       "      <th>who</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   actor  actress  credits  dory  exorcist  female  find  finding  for  from  \\\n",
       "0      0        0        0     0         0       0     0        0    0     0   \n",
       "1      0        0        1     0         0       0     0        0    1     0   \n",
       "2      1        0        0     0         1       0     0        0    0     0   \n",
       "3      0        1        0     0         0       1     1        0    0     1   \n",
       "4      0        0        0     1         0       0     0        1    0     0   \n",
       "\n",
       "   ...  on  played  plays  she  show  star  the  wars  was  who  \n",
       "0  ...   1       0      1    0     0     1    0     1    0    1  \n",
       "1  ...   0       0      0    0     1     0    1     0    0    0  \n",
       "2  ...   0       0      0    0     0     0    2     0    1    1  \n",
       "3  ...   0       0      0    1     0     0    3     0    0    0  \n",
       "4  ...   1       1      0    0     0     0    0     0    0    1  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To print the labels of the columns\n",
    "features_name = vectorizer.get_feature_names_out()\n",
    "features = vectors.toarray()\n",
    "df = pd.DataFrame(features, columns=features_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_corpus = [\n",
    "    'who was the female lead in resident evil',\n",
    "    'who played guido in life is beautiful'\n",
    "]\n",
    "\n",
    "# 'trained' vectorizer can be later used to transform the test set \n",
    "test_vectors = vectorizer.transform(test_corpus)\n",
    "print(test_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last Exercise: Text Classification\n",
    "\n",
    "- Using the Newsgroup corpus from `scikit-learn` train and evaluate a Linear SVM (LinearSVC) model on the Topic classification task\n",
    "    - [Corpus access and description](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) \n",
    "- Experiment with different vectorization methods and parameters:\n",
    "    - `binary` of Count Vectorization (CountVect)\n",
    "    - TF-IDF Transformation (TF-IDF)\n",
    "    - Using TF-IDF\n",
    "        - min and max cut-offs (CutOff)\n",
    "        - without stop-words (WithoutStopWords)\n",
    "        - without lowercasing (NoLowercase)\n",
    "\n",
    "\n",
    "**Note**:\n",
    "If the SVM doesn't converge play with the $C$ hyperparameter (starting from a small value such as 1e-4).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "# You have to generate a dev set from the training set\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
