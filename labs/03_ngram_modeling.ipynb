{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Statistical Language Modeling with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "\n",
    "- Dan Jurafsky and James H. Martin's __Speech and Language Processing__ ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 3: N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!-- ### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Ngrams and Ngram Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of $N$ items from a given sequence of text or speech. An N-gram model can model sequences, notably natural languages, by using the statistical properties of N-grams (based on N-gram counts). \n",
    "\n",
    "\n",
    "\n",
    "__Example__:\n",
    "\n",
    "- character N-grams: mice\n",
    "- word N-grams: the answer is 42\n",
    "\n",
    "\n",
    "|                     | 1-gram  | 2-gram  | 3-gram  |\n",
    "|---------------------|---------|---------|---------|\n",
    "|                     | unigram | bigram  | trigram |\n",
    "| *Markov Order*      | 0       | 1       | 2       |\n",
    "| *Character N-grams* | `['m', 'i', 'c', 'e']` | `['mi', 'ic', 'ce']` | `['mic', 'ice']` | \n",
    "| *Word N-grams*      | `['the', 'answer', 'is' , '42']` | `['the answer', 'answer is', ...]` | `['the answer is', ...]` |\n",
    "\n",
    "You can imagine this as a sliding window with a width of $N$ and a stride to the right of 1  over tokens or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Counting Ngrams\n",
    "\n",
    "*Frequency List* of a corpus is essentially a unigram count, i.e. the length of the sequences to count is 1. Indeed, N-gram count is just a generalization of *Frequency List* in which the length of the sequences to count is given by $N$ instead of only 1 as default. \n",
    "\n",
    "Thus, with N-gram count we can compute the count by taking sequences of 2 items (bigrams), 3 items (trigrams), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.1. Preparing Data\n",
    "The required data format for n-gram counting is a _list-of-lists_ (lists of sentences consisting of lists of words): \n",
    "\n",
    "```\n",
    "[\n",
    "     ['the', 'answer', 'is', '42'], \n",
    "     ['the', 'mice', 'said']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.2. Sentence beginning & end tags\n",
    "\n",
    ">Including sentence boundary markers leads to a better model. To do that we need to augment each sentence with a special symbols for beginning and end of sentence tags (`<s>` and `</s>`, respectively). The beginning of the sentence (BOS) tag gives the bigram context of the first word; and encodes probability of a word to start a sentence. Adding the end of the sentence (EOS) tag delimits the end of a sentence, i.e. it represents the final state of the sequence. The EOS is also used to signaling when the generation of a new sequence can be halted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Moreover, for n-grams larger than 2, we’ll need to assume extra context for the contexts to the left and right of the sentence boundaries. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e. `['<s>', '<s>', w1]`). Alternatively, we can use [back-off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model), and use the `['<s>', w1]` bigram probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**:\n",
    "`['<s>', 'the', 'answer', 'is', '42', '</s>']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.3. NLTK Utility Functions\n",
    "NLTK provides utility functions for padding sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent = ['the', 'answer', 'is', '42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "list(pad_sequence(\n",
    "        sent,  # input sequence\n",
    "        pad_left=True,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        pad_right=True,\n",
    "        right_pad_symbol=\"</s>\",\n",
    "        n=2  # padding for bigrams\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another NLTK function wraps this utility function with default arguments to provide a more convenient interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "list(pad_both_ends(sent, n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.4. Extracting N-grams\n",
    "NLTK provides a function to extract N-grams from a sequence, which also performs padding, if required arguments are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(sent, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bigrams = ngrams(\n",
    "    sent,\n",
    "    2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\",\n",
    ")\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Additionally, NLTK provides wrapper functions to extract bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "print(list(bigrams(sent)))\n",
    "print(list(trigrams(sent)))\n",
    "\n",
    "print(list(bigrams(pad_both_ends(sent, n=2))))\n",
    "print(list(bigrams(sent, \n",
    "                   pad_left=True, \n",
    "                   left_pad_symbol=\"<s>\", \n",
    "                   pad_right=True, \n",
    "                   right_pad_symbol=\"</s>\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.5. Extracting \"Everygrams\"\n",
    "To make an N-gram model more robust, it is usually also trained on lower-order N-grams (i.e. unigrams in case of bigrams). \n",
    "NLTK also provides an utility function to extract these together.\n",
    "\n",
    "Note the `max_len` argument that defines the maximum length of an N-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "\n",
    "print(list(everygrams(sent, max_len=3)))\n",
    "print(list(everygrams(pad_both_ends(sent, n=2), max_len=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_ngrams = everygrams(\n",
    "    sent,\n",
    "    min_len=1,\n",
    "    max_len=2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\"\n",
    ")\n",
    "list(all_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.6. \"Flattening\" the Data\n",
    "For language model training and evaluation (as well as vocabulary extraction) NLTK expects the data to be a flat list. \n",
    "In python, this is done either by using `itertools.chain.from_iterable` or python list comprehension as\n",
    "\n",
    "`[element for sublist in superlist for element in sublist]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "data = [['which', 'is', 'the', 'sense', 'of', 'life', '?'], ['the', 'answer', 'is', '42']]\n",
    "list(chain.from_iterable(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# List Comprehension\n",
    "[token for sent in data for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or another trick\n",
    "sum(data,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.7. Combining the Tasks\n",
    "NLTK wraps both tasks:\n",
    "\n",
    "- padded `ngram` (everygram) extraction from each sentence\n",
    "- flat list creation\n",
    "\n",
    "into a convenient utility function `padded_everygram_pipeline` that returns lazy iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for sent_ngrams in padded_ngrams:\n",
    "    print(list(sent_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(flat_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** lazy iterators (aka generators) are for one-use only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, ['winter is coming'.split()])\n",
    "print(padded_ngrams)\n",
    "print([list(x) for x in padded_ngrams]) # We use the generator\n",
    "print([list(x) for x in padded_ngrams]) # The generator is gone \n",
    "# If you want to reuse the generator you have to recompute it\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, ['winter is coming'.split()])\n",
    "print([list(x) for x in padded_ngrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Ngram Counter\n",
    "NLTK provides NgramCounter class to do the ngram counting. The class can be initialized without any argument and then updated with `ngrams` (via `update()` method); or directly initialized with `ngrams`.\n",
    "\n",
    "`N()` method returns total number of N-grams stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "\n",
    "counter = NgramCounter()\n",
    "counter.N()  # return total number of stored ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter.update(padded_ngrams)  # update counter with ngrams\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter = NgramCounter(padded_ngrams)\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.2.1. Accessing Ngram Counts\n",
    "\n",
    "Ngram counts can be accessed using standard python dictionary notation. \n",
    "- Ngram order counts can be accessed using integer keys (order)\n",
    "    - returns `Frequency Distribution` or `Conditional Frequency Distribution` objects. [link](https://www.nltk.org/api/nltk.probability.html)\n",
    "- Unigram counts can be accessed using string keys.\n",
    "- Bigram counts can be accessed using list keys & string keys (see example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# ngram order counts\n",
    "print(counter[1])  # Frequency Distribution\n",
    "print(counter[2])  # Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# print counts\n",
    "print(counter.unigrams)  # unigram count for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# unigram counts\n",
    "counter['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 'full' bigram counts (full bigrams)\n",
    "counter[['the']]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# to get a frequency distribution over all continuations, you can use list or tuple. \n",
    "# counter[['the']] or counter[('the',)]\n",
    "counter[['the']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# For Conditional Frequency Distributions, only tuples are accepted.\n",
    "# counter[2][['the']] with throw an error\n",
    "counter[2][('the',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "- Load Shakespeare's Hamlet from Gutenberg corpus\n",
    "    - lowercase it\n",
    "\n",
    "- Extract padded unigrams and bigrams\n",
    "\n",
    "- Using NgramCounter\n",
    "    - get total number of ngrams\n",
    "    - get count of unigram `the`\n",
    "    - get count of bigram `of the`\n",
    "    \n",
    "|                     | Count  | \n",
    "|---------------------|---------|\n",
    "| Ngrams      | 84038     | \n",
    "| Unigram *the* | 993|\n",
    "| Bigram *of the*     |59 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# It returns a list of lists\n",
    "hamlet = gutenberg.sents('shakespeare-hamlet.txt')\n",
    "\n",
    "print(len(hamlet))\n",
    "\n",
    "# lowercasing use .lower()\n",
    "hamlet_lowercase = #lowercase the hamlet corpus\n",
    "\n",
    "# double check\n",
    "print(hamlet[0])\n",
    "print(hamlet_lowercase[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_ngrams, flat_text = #use the padded every gram pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "counter = # NgramCounter(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter) # Total number of Ngrams \n",
    "print(counter[]) # the\n",
    "print(counter[[]][]) # of the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Vocabulary and Basic Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[`Vocabulary`](https://www.nltk.org/api/nltk.lm.vocabulary.html) class of NLTK satisfies two common language modeling requirements for a vocabulary:\n",
    "- When checking membership and calculating its size, filters items by comparing their counts to a cutoff value.\n",
    "- Adds a special \"unknown\" token which unseen words are mapped to.\n",
    "\n",
    "Tokens with counts greater than or equal to the cut-off value will be considered part of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary(counts=None, unk_cutoff=1, unk_label='<UNK>)` create a new `Vocabulary`.\n",
    "\n",
    "- `counts` (optional) iterable or collections. \n",
    "    - Counter instance to pre-seed the `Vocabulary`. \n",
    "    - In case it is iterable, counts are calculated.\n",
    "\n",
    "- `unk_cutoff` (int) - Words that occur less frequently than this value are not considered part of the vocabulary.\n",
    "\n",
    "- `unk_label` - Label for marking words not part of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "hamlet_words = gutenberg.words('shakespeare-hamlet.txt')\n",
    "\n",
    "# lowercase\n",
    "hamlet_words = [w.lower() for w in hamlet_words]\n",
    "\n",
    "# initialize vocabulary with cut-off\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary` methods:\n",
    "\n",
    "- `lookup()` looks up words in a vocabulary. \n",
    "    - \"Unseen\" words (with counts less than cutoff) are looked up as the unknown label. \n",
    "    - If given one word (a string) as an input, this method will return a string.\n",
    "    - If given a sequence, it will return an tuple of the looked up words.\n",
    "    \n",
    "- `update()` update counts from a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`Vocabulary` properties:\n",
    "\n",
    "- `cutoff` - same as `unk_cutoff`\n",
    "\n",
    "- `unk_label` and `counts` can also be accessed as properties\n",
    "    - `vocab.unk_label`\n",
    "    - `vocab.counts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Counts, Vocabulary Membership, and Lookup\n",
    "\n",
    "Tokens with frequency counts less than the `cutoff` value will be considered not part of the vocabulary, even though their entries in the count dictionary are preserved. \n",
    "\n",
    "This is useful for changing cut-off without recomputing counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's define a function to illustrate the relation between counts, membership, and lookup\n",
    "def test_word(word, vocab):\n",
    "    # let's lowercase it\n",
    "    if word != vocab.unk_label:\n",
    "        word = word.lower() \n",
    "    # membership test\n",
    "    word_membership = word in vocab\n",
    "    # accessing count\n",
    "    word_count = vocab[word]\n",
    "    # lookup\n",
    "    word_lookup = vocab.lookup(word)\n",
    "    \n",
    "    print(\"Word: '{}', Count: {}, In Vocab: {}, Mapped to: '{}'\".format(\n",
    "        word, word_count, word_membership, word_lookup))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for w in [\"<UNK>\", \"the\", \"1599\", \"Trento\"]:\n",
    "    test_word(w, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Cut-Off and Vocabulary Size\n",
    "The cut-off value influences not only membership checking but also the result of getting the size of the vocabulary using the built-in `len`. Note that while the number of keys in the vocabulary's counter remains the same, the items in the vocabulary differ depending on the cut-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define another vocabulary with cut-off 1\n",
    "# Notice that we are passing counts from the original vocabulary\n",
    "vocab1 = Vocabulary(vocab.counts, unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"CutOff 2:\", len(vocab))\n",
    "print(\"CutOff 1:\", len(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"CutOff 2 Counts:\", len(vocab.counts))\n",
    "print(\"CutOff 1 Counts:\", len(vocab1.counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "- lookup in vocabulary\n",
    "    - \"trento is the capital city of trentino\"\n",
    "- update vocabulary with \"trento is the capital city of trentino\"\n",
    "    - do the lookup again to see the effect\n",
    "- experiment with changing the cut-off value from `1` to `10`\n",
    "    - do the lookup again to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"trento is the capital city of trentino\" # use the .split() to tokenize the sentence\n",
    "\n",
    "# Cut-off 0 \n",
    "print('Cut-off 0')\n",
    "vocab = Vocabulary(hamlet_words)\n",
    "# Print lookup before vocab update using vocab.lookup\n",
    "# Update vocab using vocab.update\n",
    "# Print lookup after vocab update using vocab.lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cut-off 1')\n",
    "# Cut-off 1\n",
    "# Reinitialize vocab with unk_cutoff= 1\n",
    "# Print lookup before vocab update\n",
    "# Update vocab\n",
    "# Print lookup after vocab update\n",
    "\n",
    "print('Cut-off 10')\n",
    "# Cut-off 10\n",
    "# Reinitialize vocab with unk_cutoff= 10\n",
    "# Print lookup before vocab update\n",
    "# Update vocab\n",
    "# Print lookup after vocab update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Training Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the data prepared as we have seen above, the `NLTK` ngram model boils down to counting ngrams, as presented above, by specifying the highest ngram size order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's prepare data on hamlet\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "data = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's train a `Maximum Likelihood Estimator (MLE)\n",
    "from nltk.lm import MLE\n",
    "\n",
    "mle_lm = MLE(2) # Where 2 is the highest N-gram size order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Initialization of the Language Model creates:\n",
    "- empty vocabulary\n",
    "- empty counts\n",
    "\n",
    "These are populated once we `fit` the model with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# .vocab refers to the Vocabulary class that we have seen before\n",
    "print(mle_lm.vocab)\n",
    "# .vocab refers to the NgramCounter class that we have seen before\n",
    "print(mle_lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mle_lm.fit(padded_ngrams, flat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(mle_lm.vocab)\n",
    "print(mle_lm.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Using Ngram Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $n$, it assigns a probability $P(w_{1},\\ldots ,w_{n})$ ($P(w_{1}^{n})$, for compactness) to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute an ngram language model of our corpus.\n",
    "\n",
    "<!-- It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Computing Probability of a Sequence (Scoring)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Probability of a Sequence\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities ([Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability))). \n",
    "\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "The order of ngram makes a simplifying assumption that probability of a current word only depends on previous $N - 1$ elements. Thus, it truncates previous context (history) to length $N - 1$.\n",
    "\n",
    "$$P(w_i|w_{1}^{i-1}) \\approx P(w_i|w_{i-N+1}^{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\\begin{align*}\n",
    "Unigram:\\;& P(w_i) \\\\\n",
    "Bigram: \\;& P(w_i|w_{i-1}) \\\\\n",
    "Trigram: \\;& P(w_i|w_{i-2},w_{i-1}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<!--| N-gram   | Equation                     |\n",
    ":--------:|:---------------------------:|\n",
    "unigram  | $P(w_i)$                     |\n",
    "bigram   | $P(w_i|w_{i-1})$             |\n",
    "trigram  | $P(w_i|w_{i-2},w_{i-1})$     | \n",
    "-->\n",
    "The probability of the whole sequence applying an ngram model becomes:\n",
    "\n",
    "$$P(w_{1}^{n}) = \\prod_{i=1}^{n}{P(w_i|w_{i-N+1}^{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of ngrams can be computed by *normalizing* frequency counts (aka *Maximum Likelihood Estimation*): dividing the frequency of an ngram sequence by the frequency of its prefix (*relative frequency*).\n",
    "\\begin{align*}\n",
    "Unigram:\\; & P(w_i) = \\frac{c(w_i)}{N}\\\\\n",
    "Bigram:\\; & P(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}\\\\\n",
    "Ngram: \\; & P(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "<!-- \n",
    "N-gram   | Equation                      \n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$ \n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$ \n",
    "Ngram    | $$p(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}$$  -->\n",
    "\n",
    "\n",
    "where:\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus (x could be unigram, bigram, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.1. Scoring Methods of NLTK LMs\n",
    "- `score(word, context=None)` masks out of vocab (OOV) words (i.e. maps them to `<UNK>`) and computes their model score.\n",
    "    - scores are **model-specific** (later on this)\n",
    "    - Parameters:\n",
    "        - `word (str)` - Word for which we want the score\n",
    "        - `context (tuple(str))` - Context the word is in. If None, computes unigram score.\n",
    "        \n",
    "- `logscore(word, context=None)` - Evaluate the log score of this word in this context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# A little illustration of the methods\n",
    "mle_lm.score(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mle_lm.score(\"the\", [\"of\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import math \n",
    "mle_lm.logscore(\"the\", [\"of\"]) # == math.log(mle_lm.logscore(\"the\", [\"of\"]), 2) log base 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "Implement a function to compute score of a sequence (i.e. Chain Rule)\n",
    "\n",
    "- arguments:\n",
    "    - Language Model\n",
    "    - List of Tokens\n",
    "\n",
    "- functionality\n",
    "    - extracts ngrams w.r.t. LM order (`lm.order`)\n",
    "    - scores each ngram w.r.t. LM (`lm.score` or `lm.logscore`)\n",
    "        - note: `score` already takes care of OOV by converting to `<UNK>` \n",
    "    - computes the overall score using chain rule\n",
    "        - note: the difference between `score` and `logscore`\n",
    "\n",
    "- compute the scores of the `test_sents`\n",
    "    - compute the scores of padded and unpadded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Toy test set\n",
    "test_sents = [\"the king is dead\", \"the tzar is dead\", 'the tragedie of hamlet is good']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def chain_rule(lm, sentence, log=True, pad=True):\n",
    "    highest_ngram = lm.order\n",
    "    tokens = #split sentence in tokens w .split\n",
    "    if pad:\n",
    "        ngrams, _ = #use padded everygram pipeline with the ngram is the highest ngram\n",
    "        ngrams = chain.from_iterable(ngrams) # Flat the sequence\n",
    "    else:\n",
    "        ngrams = # use everygrams with max len= highest ngram \n",
    "  \n",
    "    \n",
    "    if log:\n",
    "        total_score = 0\n",
    "    else:\n",
    "        total_score = 1\n",
    "        \n",
    "    for x in ngrams:\n",
    "        # x: (w_t-N, w_t-(N-1), w_t-(N-2), ... ,w_t)\n",
    "        if len(x) == highest_ngram:\n",
    "            if log:\n",
    "                w_t = x[-1]\n",
    "                # In python you can get a split of a tuple or array as array[from:to] \"to\" is excluded\n",
    "                context = # get the slice from 0 to -2\n",
    "                score = lm.logscore(w_t, context)\n",
    "                total_score # Add or multiply score to total_score ?\n",
    "            else:\n",
    "                w_t = x[-1]\n",
    "                # In python you can get a split of a tuple or array as array[from:to] \"to\" is excluded\n",
    "                context = # get the slice from 0 to -2 \n",
    "                score = lm.score(w_t, context)\n",
    "                total_score # Add or multiply score to total_score ?\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "for sent in test_sents:\n",
    "    print(sent, chain_rule(mle_lm, sent, log=True, pad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.2. OOV in MLE\n",
    "In MLE LM we did not take care of OOV. Consequently, those have `0` counts and probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(mle_lm.score(\"<UNK>\"))\n",
    "print(mle_lm.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(mle_lm.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# same as above: getting lowercaseed sentences and words \n",
    "hamlet_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "hamlet_words = flatten(hamlet_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# computing vocabulary with cutoff\n",
    "lex = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "print(len(lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# replacing words with counts below cutoff with '<UNK>'\n",
    "hamlet_oov_sents = [list(lex.lookup(sent)) for sent in hamlet_sents]\n",
    "print(hamlet_oov_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# extracting ngrams & words again\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(2, hamlet_oov_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lm_oov = MLE(2)\n",
    "lm_oov.fit(padded_ngrams_oov, flat_text_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(lm_oov.score(\"<UNK>\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(lm_oov.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.3. Data Sparsity: Missing Ngrams\n",
    "However, even though we have counted ngrams on the data set with `<UNK>`, the counts still do not account for all possible ngrams. Thus, some possible sequences will have zero probability.\n",
    "\n",
    "__We will address this later.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(lm_oov.score(\"queen\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"queen\"]))\n",
    "print(lm_oov.score(\"queen\", [\"<UNK>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Generating Sequences\n",
    "\n",
    "Ngram Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "__Algorithm for Bigram LM__\n",
    "\n",
    "- $w_{i-1} = $ `<s>`;\n",
    "- *while* $w_i \\neq $ `</s>`\n",
    "\n",
    "    - stochastically get new word w.r.t. $P(w_i|w_{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.1. Generation Method of NLTK LM\n",
    "- `generate(num_words=1, text_seed=None, random_seed=None)` generates words from the model.\n",
    "    - Parameters\n",
    "        - `num_words (int)` - How many words to generate. By default 1.\n",
    "        - `text_seed` - preceding context the generation should be conditioned on.\n",
    "            - ngram model is restricted in how much preceding context it can take into account based on max ngram size\n",
    "        - `random_seed` - A random seed. If provided, makes the random sampling part of generation reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mle_lm.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Note that you have to manually truncate the sentence when there is a </s>\n",
    "mle_lm.generate(5, text_seed=[\"<s>\", \"hamlet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3. Language Model Evaluation\n",
    "\n",
    "Language Models are evaluated using [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data\n",
    "- Probability of test data\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "\\begin{align*}\n",
    " PP(W) = \\sqrt[N]{\\frac{1}{P(w_1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}P(w_i|w_{i-N+1})}}\n",
    " \\end{align*}\n",
    "\n",
    "(abbreviated as PPL also)\n",
    "\n",
    "Where $N$ is the number of words in test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.3.1. Evaluation Methods of NLTK LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`NLTK` provides both **perplexity** and **cross-entropy** as evaluation methods\n",
    "\n",
    "- `entropy(text_ngrams)` calculates cross-entropy of model for given evaluation text.\n",
    "    - Parameters\n",
    "        - `text_ngrams (Iterable(tuple(str)))` A sequence of ngram tuples.\n",
    "\n",
    "- `perplexity(text_ngrams)` calculates the perplexity of the given text.\n",
    "    - This is 2<sup>cross-entropy</sup> for the text, so the arguments are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Perplexity** is related to [**Cross-Entropy**](https://en.wikipedia.org/wiki/Cross_entropy) as:\n",
    "\\begin{align*}\n",
    "PP(p) = 2^{H(p)} \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 4\n",
    "Compute entropy and perplexity of the `MLE` models  on the bigrams of the test sentences below, treating them as a test set.\n",
    "\n",
    "- experiment with the two test sets\n",
    "- experiment with OOVs (with vs without)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_sents1 = [\"the king is dead\", \"the emperor is dead\", \"may the force be with you\"]\n",
    "test_sents2 = [\"the king is dead\", \"welcome to you\", \"how are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "hamlet_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "hamlet_words = flatten(hamlet_sents)\n",
    "# Compute vocab \n",
    "lex = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "# Handeling OOV\n",
    "hamlet_oov_sents = [list(lex.lookup(sent)) for sent in hamlet_sents]\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(2, hamlet_oov_sents)\n",
    "# Train the model \n",
    "lm_oov = MLE(2)\n",
    "lm_oov.fit(padded_ngrams_oov, flat_text_oov)\n",
    "# Compute PPL and entropy with OOV on test 1\n",
    "test_set = test_sents2\n",
    "\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "# Compute PPL extracting ngram equals to lm_oov.order\n",
    "# print(lm_oov.perplexity([# Extract ngrams == lm_oov.order))\n",
    "\n",
    "# Generators are one-use only!\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "# Compute Entropy extracting ngram equals to lm_oov.order\n",
    "# print(lm_oov.entropy([# Extract ngrams == lm_oov.order ])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPL: how it works inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def compute_ppl(model, data):\n",
    "    highest_ngram = model.order\n",
    "    scores = [] \n",
    "    for sentence in data:\n",
    "        ngrams, _ = padded_everygram_pipeline(highest_ngram, [sentence.split()])\n",
    "        scores.extend([-1.0 * model.logscore(w[-1], w[0:-1]) for gen in ngrams for w in gen if len(w) == highest_ngram])\n",
    "    return math.pow(2.0, np.asarray(scores).mean())\n",
    "compute_ppl(mle_lm, test_sents2)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.4. Handling Data Sparseness: Smoothing in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[`Smoothing(vocabulary, counter, **kwargs)`](https://www.nltk.org/api/nltk.lm.smoothing.html) Class is initialized with the following parameters: \n",
    "\n",
    "- `vocabulary (nltk.lm.vocab.Vocabulary)` - The Ngram vocabulary object.\n",
    "- `counter (nltk.lm.counter.NgramCounter)` - The counts of the vocabulary items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Ngram Smoothing Interface` implements [Chen & Goodman 1995](https://aclanthology.org/P96-1041.pdf)'s idea that all smoothing algorithms have certain features in common. Consequently, each Smoothing subclass implements two methods:\n",
    "\n",
    "- `unigram_score(word)` return unigram score\n",
    "- `alpha_gamma(word, context)` returns alpha and gamma values (varies w.r.t. method)\n",
    "    - `gamma` is the value added to missing ngram count\n",
    "    - `alpha` is the value used to scale the lower order probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following smoothing methods are implemented:\n",
    "- `WittenBell(vocabulary, counter, **kwargs)` - Witten-Bell smoothing\n",
    "\n",
    "- `AbsoluteDiscounting(vocabulary, counter, discount=0.75, **kwargs)` - Smoothing with absolute discount\n",
    "    - takes `discount=0.75` parameter (default 0.75)\n",
    "    \n",
    "- `KneserNey(vocabulary, counter, order, discount=0.1, **kwargs)` - Kneser-Ney Smoothing (an extension of smoothing with a discount)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5. NLTK Language Models\n",
    "\n",
    "All the language models in NLTK share the same interface and share methods for\n",
    "- evaluation\n",
    "- generation\n",
    "- scoring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Scoring** in a language model does the following:\n",
    "- takes care of OOV words (see above)\n",
    "- computes `unmasked_score`\n",
    "\n",
    "The way `unmasked_score` is computed from counts is the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some language models use **smoothing** to account for missing ngrams, some do not (e.g. `MLE`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Available LM Implementations\n",
    "\n",
    "- `MLE` - providing MLE ngram model scores.\n",
    "\n",
    "- Additive Smoothing LMs\n",
    "    - `Laplace` - Laplace (add one) smoothing ($\\gamma = 1$)\n",
    "    - `Lidstone` - same as `Laplace`, but adds $\\gamma$\n",
    "\n",
    "- Back-Off Language Models\n",
    "    - `StupidBackoff` - uses `alpha` to scale the lower order probabilities for computing missing ngram scores\n",
    "\n",
    "- Interpolated Language Models\n",
    "    - `WittenBellInterpolated` - Interpolated version of Witten-Bell smoothing\n",
    "    - `KneserNeyInterpolated` - Interpolated version of Kneser-Ney smoothing.\n",
    "    - `AbsoluteDiscountingInterpolated` - Interpolated version of smoothing with absolute discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last Exercise: Language Model Evaluation\n",
    "Write your own implementation of the Stupid backoff algorithm. Train it and compare the perplexity with the one provided by NLKT. The dataset that you have to use is the *Shakespeare Macbeth*. You have to split the dataset in training, development and test sets. The train the model on the training set, find the best ⍺ on the dev set, and test the model on the test set.\n",
    "\n",
    "Stupid Backoff algorithm (use ⍺=0.4):\n",
    "https://aclanthology.org/D07-1090.pdf \n",
    "\n",
    "NLTK (StupidBackoff):\n",
    "https://www.nltk.org/api/nltk.lm.html\n",
    "\n",
    "**Suggestion**: adapt the `compute_ppl` function to compute the perplexity of your model. The PPL has to be computed on the whole corpus and not at sentence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# NLTK StupidBackoff\n",
    "from nltk.lm import StupidBackoff\n",
    "# Dataset\n",
    "macbeth_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-macbeth.txt')]\n",
    "# Split the corpus 'shakespeare-macbeth.txt' into train, dev, test as we have seen in LAB 02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
