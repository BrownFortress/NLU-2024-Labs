{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Sense Disambiguation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises 1\n",
    "Even though NLTK states that it implements Original Lesk Algorithm, in fact it is a Simplified Lesk Algorithm, that doesn't consider examples, and computes overlaps like the original. \n",
    "\n",
    "In the original algorithm context is computed differently. <mark style=\"background-color: rgba(0, 255, 0, 0.2)\">Instead of comparing a target word's signature with the context words, the target signature is compared with the signatures of each of the context words. </mark>\n",
    "\n",
    "Implement the Original Lesk Algorithm (modifying NLTK's, see pseudocode above)\n",
    "Todo list:\n",
    "- Complete lesk simplified\n",
    "- Preprocessing:\n",
    "    - compute pos-tag with `nltk.pos_tag`\n",
    "    - remove stopwords\n",
    "        - `from nltk.corpus import stopwords`\n",
    "        - `stopwords.words('english')`\n",
    "\n",
    "- take the majority decision (the sense predicted most frequently)\n",
    "\n",
    "POS tags reminder:\n",
    "\n",
    "| POS | in Synset Name |\n",
    "|:----|:---------------|\n",
    "| `wn.NOUN`    | `n`\n",
    "| `wn.VERB`    | `v`\n",
    "| `wn.ADV`     | `r`\n",
    "| `wn.ADJ`     | `a`\n",
    "| `wn.ADJ_SAT` | `s` (satelite adjective, ignore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(text):\n",
    "    mapping = {\"NOUN\": wordnet.NOUN, \"VERB\": wordnet.VERB, \"ADJ\": wordnet.ADJ, \"ADV\": wordnet.ADV}\n",
    "    sw_list = stopwords.words('english')\n",
    "    \n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    # tokenize, if input is text\n",
    "    tokens = nltk.word_tokenize(text) if type(text) is str else text\n",
    "    # pos-tag\n",
    "    tagged = nltk.pos_tag(tokens, tagset=\"universal\")\n",
    "    # lowercase\n",
    "    tagged = [(w.lower(), p) for w, p in tagged]\n",
    "    # optional: remove all words that are not NOUN, VERB, ADJ, or ADV (i.e. no sense in WordNet)\n",
    "    tagged = [(w, p) for w, p in tagged if p in mapping]\n",
    "    # re-map tags to WordNet (return orignal if not in-mapping, if above is not used)\n",
    "    tagged = [(w, mapping.get(p, p)) for w, p in tagged]\n",
    "    # remove stopwords\n",
    "    tagged = [(w, p) for w, p in tagged if w not in sw_list]\n",
    "    # lemmatize\n",
    "    tagged = [(w, lem.lemmatize(w, pos=p), p) for w, p in tagged]\n",
    "    # unique the list\n",
    "    tagged = list(set(tagged))\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense_definitions(context):\n",
    "    # input is text or list of strings\n",
    "    lemma_tags = preprocess(context)\n",
    "    # let's get senses for each\n",
    "    senses = [(w, wordnet.synsets(l, p)) for w, l, p in lemma_tags]\n",
    "    \n",
    "    # let's get their definitions\n",
    "    definitions = []\n",
    "    for raw_word, sense_list in senses:\n",
    "        if len(sense_list) > 0:\n",
    "            # let's tokenize, lowercase & remove stop words \n",
    "            def_list = []\n",
    "            for s in sense_list:\n",
    "                defn = s.definition()\n",
    "                # let's use the same preprocessing\n",
    "                tags = preprocess(defn)\n",
    "                toks = [l for w, l, p in tags]\n",
    "                def_list.append((s, toks))\n",
    "            definitions.append((raw_word, def_list))\n",
    "    return definitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sense(words, sense_list):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    val, sense = max((len(set(words).intersection(set(defn))), ss) for ss, defn in sense_list)\n",
    "    return val, sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def lesk_simplified(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
    "    context = set(context_sentence)\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = wordnet.synsets(ambiguous_word)\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    # Measure the overlap between context and definitions\n",
    "    _, sense = max(\n",
    "        (len(context.intersection(ss.definition().split())), ss) for ss in synsets\n",
    "    )\n",
    "\n",
    "    return sense\n",
    "\n",
    "def original_lesk(context_sentence, ambiguous_word, pos=None, synsets=None, majority=False):\n",
    "\n",
    "    context_senses = get_sense_definitions(set(context_sentence)-set([ambiguous_word]))\n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    scores = []\n",
    "    # print(synsets)\n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            scores.append(get_top_sense(sense[1], synsets))\n",
    "            \n",
    "    if len(scores) == 0:\n",
    "        return synsets[0][0]\n",
    "    \n",
    "    if majority:\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]\n",
    "    else:\n",
    "        _, best_sense = max(scores)\n",
    "    return best_sense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jane sat on the sloping bank of a river beside the water\".split()\n",
    "word = \"bank\"\n",
    "print(\"Sense from lesk original\", original_lesk(text, word, majority=True))\n",
    "print(\"Sense from lesk simplified\", lesk_simplified(text, word))\n",
    "print(\"Sense from lesk NLTK\", lesk(text, word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2\n",
    "Extend Lesk algorithm (function) to use similarity metrics instead of just overlaps\n",
    "- add a keyword argument to allow different metrics\n",
    "\n",
    "Complete the Predersen algorithm with the similiratiy metrics\n",
    "- compare the output of the two algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "def get_top_sense_sim(context_sense, sense_list, similarity):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    scores = []\n",
    "    for sense in sense_list:\n",
    "        ss = sense[0]\n",
    "        if similarity == \"path\":\n",
    "            try:\n",
    "                scores.append((context_sense.path_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))    \n",
    "        elif similarity == \"lch\":\n",
    "            try:\n",
    "                scores.append((context_sense.lch_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"wup\":\n",
    "            try:\n",
    "                scores.append((context_sense.wup_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"resnik\":\n",
    "            try:\n",
    "                scores.append((context_sense.res_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"lin\":\n",
    "            try:\n",
    "                scores.append((context_sense.lin_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"jiang\":\n",
    "            try:\n",
    "                scores.append((context_sense.jcn_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        else:\n",
    "            print(\"Similarity metric not found\")\n",
    "            return None\n",
    "    val, sense = max(scores)\n",
    "    return val, sense\n",
    "\n",
    "def lesk_similarity(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, \n",
    "                    synsets=None, majority=True):\n",
    "    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # Here you may have some room for improvement\n",
    "    # For instance instead of using all the definitions from the context\n",
    "    # you pick the most common one of each word (i.e. the first)\n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            scores.append(get_top_sense_sim(sense[0], synsets, similarity))\n",
    "            \n",
    "    if len(scores) == 0:\n",
    "        return synsets[0][0]\n",
    "    \n",
    "    if majority:\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]\n",
    "    else:\n",
    "        _, best_sense = max(scores)\n",
    "    \n",
    "    return best_sense\n",
    "\n",
    "def pedersen(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, \n",
    "                    synsets=None, threshold=0.1):\n",
    "    \n",
    "    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\n",
    "\n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    synsets_scores = {}\n",
    "    for ss_tup in synsets:\n",
    "        ss = ss_tup[0]\n",
    "        if ss not in synsets_scores:\n",
    "            synsets_scores[ss] = 0\n",
    "        for senses in context_senses:\n",
    "            scores = []\n",
    "            for sense in senses[1]:\n",
    "                if similarity == \"path\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].path_similarity(ss), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))    \n",
    "                elif similarity == \"lch\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].lch_similarity(ss), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"wup\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].wup_similarity(ss), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"resnik\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].res_similarity(ss, semcor_ic), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"lin\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].lin_similarity(ss, semcor_ic), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"jiang\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].jcn_similarity(ss, semcor_ic), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                else:\n",
    "                    print(\"Similarity metric not found\")\n",
    "                    return None\n",
    "            value, sense = max(scores)\n",
    "            if value > threshold:\n",
    "                synsets_scores[sense] = synsets_scores[sense] + value\n",
    "    \n",
    "    values = list(synsets_scores.values())\n",
    "    if sum(values) == 0:\n",
    "        print('Warning all the scores are 0')\n",
    "    senses = list(synsets_scores.keys())\n",
    "    best_sense_id = values.index(max(values))\n",
    "    return senses[best_sense_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jane sat on the sloping bank of a river beside the water\".split()\n",
    "word = \"bank\"\n",
    "sense = original_lesk(text, word, majority=True)\n",
    "print('Original lesk', sense, sense.definition())\n",
    "sense = lesk(text, word)\n",
    "print('Symplified lesk', sense, sense.definition())\n",
    "sense = predersen(text, word, similarity=\"resnik\", threshold=0.1)\n",
    "print(\"Pedersen\", sense, sense.definition())\n",
    "sense = lesk_similarity(text, word, pos='n', similarity=\"path\")\n",
    "print('Graph-based lesk', sense, sense.definition())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 3\n",
    "- Evaluate Original Lesk (your implementation on Senseval's `interest`)\n",
    "- You can also easily evaluate Lesk similarity that we have seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import precision, recall, f_measure, accuracy\n",
    "\n",
    "refs = {k: set() for k in mapping.values()}\n",
    "hyps = {k: set() for k in mapping.values()}\n",
    "refs_list = []\n",
    "hyps_list = []\n",
    "\n",
    "# since WordNet defines more senses, let's restrict predictions\n",
    "\n",
    "synsets = []\n",
    "for ss in wordnet.synsets('interest', pos='n'):\n",
    "    if ss.name() in mapping.values():\n",
    "        defn = ss.definition()\n",
    "        tags = preprocess(defn)\n",
    "        toks = [l for w, l, p in tags]\n",
    "        synsets.append((ss,toks))\n",
    "\n",
    "for i, inst in enumerate(senseval.instances('interest.pos')):\n",
    "    txt = [t[0] for t in inst.context]\n",
    "    raw_ref = inst.senses[0] # let's get first sense\n",
    "    hyp = original_lesk(txt, txt[inst.position], synsets=synsets, majority=True).name()\n",
    "    ref = mapping.get(raw_ref)\n",
    "    \n",
    "    # for precision, recall, f-measure        \n",
    "    refs[ref].add(i)\n",
    "    hyps[hyp].add(i)\n",
    "    \n",
    "    # for accuracy\n",
    "    refs_list.append(ref)\n",
    "    hyps_list.append(hyp)\n",
    "\n",
    "print(\"Acc:\", round(accuracy(refs_list, hyps_list), 3))\n",
    "\n",
    "for cls in hyps.keys():\n",
    "    p = precision(refs[cls], hyps[cls])\n",
    "    r = recall(refs[cls], hyps[cls])\n",
    "    f = f_measure(refs[cls], hyps[cls], alpha=1)\n",
    "    \n",
    "    print(\"{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}\".format(cls, p, r, f, len(refs[cls])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
