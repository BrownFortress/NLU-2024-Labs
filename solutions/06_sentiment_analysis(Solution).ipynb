{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "- Apply [Negation Marking](https://www.nltk.org/_modules/nltk/sentiment/util.html#mark_negation) to Movie Reviews Dataset\n",
    "    - expects list as input\n",
    "- Train SVM model\n",
    "- Compare results to Naive Bayes performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'not', 'cool_NEG']\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.util import mark_negation\n",
    "print(mark_negation(\"This is not cool\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.959\n",
      "NB: 0.961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "new_neg = []\n",
    "for rev in rev_neg:\n",
    "    new_rev = []\n",
    "    for sentence in rev:\n",
    "        new_rev.append(mark_negation(sentence))\n",
    "    new_neg.append(new_rev)\n",
    "\n",
    "new_pos = []\n",
    "for rev in rev_pos:\n",
    "    new_rev = []\n",
    "    for sentence in rev:\n",
    "        new_rev.append(mark_negation(sentence))\n",
    "    new_pos.append(new_rev)\n",
    "    \n",
    "new_corpus = [lol2str(d) for d in new_neg] + [lol2str(d) for d in rev_pos]\n",
    "vectors = vectorizer.fit_transform(new_corpus)\n",
    "ref = numpy.array([0] * len(new_neg) + [1] * len(new_pos))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores_svm = []\n",
    "scores_nb = []\n",
    "for i, (train_index, test_index) in enumerate(skf.split(new_corpus, ref)):\n",
    "    x_train, x_test = [new_corpus[indx] for indx in train_index], [new_corpus[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    svm_classifier = LinearSVC(C=100, max_iter=10000, dual=True)\n",
    "    nb_classifier = MultinomialNB()\n",
    "    \n",
    "    clf_svm = svm_classifier.fit(train_features, y_train)\n",
    "    hyp_svm = clf_svm.predict(test_features)\n",
    "#     print(classification_report(y_test, hyp_svm))\n",
    "    scores_svm.append(f1_score(y_test, hyp_svm, average='macro'))\n",
    "    clf_nb = nb_classifier.fit(train_features, y_train)\n",
    "    hyp_nb = clf_nb.predict(test_features)\n",
    "#     print(classification_report(y_test, hyp_nb))\n",
    "    scores_nb.append(f1_score(y_test, hyp_nb, average='macro'))\n",
    "    \n",
    "print('SVM:', round(sum(scores_svm)/len(scores_svm), 3))\n",
    "print('NB:', round(sum(scores_nb)/len(scores_nb), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Implement classification using counts of negative and positive terms (i.e. convert scores to label at token-level and count those for a document)\n",
    "2. Implement classification using counts of negative and positive sentences (i.e. score sentences and aggregate their labels)\n",
    "3. Train and evaluate supervise machine learning model by first removing objective sentences.\n",
    "4. Classify Movie Reviews using VADER.\n",
    "5. Do a proper cross-validation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD LEVEL\n",
      "=========================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.73      0.32      0.44      1000\n",
      "           P       0.56      0.88      0.69      1000\n",
      "\n",
      "    accuracy                           0.60      2000\n",
      "   macro avg       0.65      0.60      0.57      2000\n",
      "weighted avg       0.65      0.60      0.57      2000\n",
      "\n",
      "=========================================================================================\n",
      "SENTENCE LEVEL\n",
      "=========================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.72      0.39      0.51      1000\n",
      "           P       0.58      0.85      0.69      1000\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.65      0.62      0.60      2000\n",
      "weighted avg       0.65      0.62      0.60      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"WORD LEVEL\")\n",
    "print(\"=\"*89)\n",
    "def polarity_word_level(document, analyzer):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    labels = ['P', 'N']\n",
    "    for sentence in document:\n",
    "        for w in sentence:\n",
    "            value = analyzer.polarity_scores(w)\n",
    "            if value[\"compound\"] > 0:\n",
    "                pos += 1 \n",
    "            elif value[\"compound\"] < 0:\n",
    "                neg += 1\n",
    "    return labels[np.argmax(np.asarray([pos, neg]))]\n",
    "\n",
    "hyp_word_level =  [polarity_word_level(doc, analyzer) for doc in rev_neg] + \\\n",
    "                  [polarity_word_level(doc, analyzer) for doc in rev_pos]\n",
    "print(classification_report(ref, hyp_word_level))    \n",
    "print(\"=\"*89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE LEVEL\n",
      "=========================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.72      0.39      0.51      1000\n",
      "           P       0.58      0.85      0.69      1000\n",
      "\n",
      "    accuracy                           0.62      2000\n",
      "   macro avg       0.65      0.62      0.60      2000\n",
      "weighted avg       0.65      0.62      0.60      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SENTENCE LEVEL\")\n",
    "print(\"=\"*89)\n",
    "\n",
    "def polarity_sentence_level(document, analyzer):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    labels = ['P', 'N']\n",
    "    for sentence in document:\n",
    "        value = analyzer.polarity_scores(\" \".join(sentence))\n",
    "        if value[\"compound\"] > 0:\n",
    "            pos += 1 \n",
    "        elif value[\"compound\"] < 0:\n",
    "            neg += 1\n",
    "    return labels[np.argmax(np.asarray([pos, neg]))]\n",
    "\n",
    "hyp_sentence_level =  [polarity_sentence_level(doc, analyzer) for doc in rev_neg] + \\\n",
    "                  [polarity_sentence_level(doc, analyzer) for doc in rev_pos]\n",
    "print(classification_report(ref, hyp_sentence_level))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def rm_objective_sentences(document, analyzer):\n",
    "    new_doc = []\n",
    "    for sentence in document:\n",
    "        value = analyzer.polarity_scores(\" \".join(sentence))\n",
    "        if value[\"compound\"] != 0:\n",
    "            new_doc.append(\" \".join(sentence))\n",
    "    return new_doc\n",
    "def polarity_doc_level(document, analyzer):\n",
    "    value = analyzer.polarity_scores(document)\n",
    "    if value[\"compound\"] > 0:\n",
    "        return 'P'\n",
    "    elif value[\"compound\"] <= 0: # In this way we penalize the neg class\n",
    "        return 'N'\n",
    "    \n",
    "rev_neg_wo_objective = [\"\\n\".join(rm_objective_sentences(doc, analyzer)) for doc in rev_neg]\n",
    "rev_pos_wo_objective = [\"\\n\".join(rm_objective_sentences(doc, analyzer)) for doc in rev_pos]\n",
    "corpus_wo_objective = rev_neg_wo_objective + rev_pos_wo_objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 classifier: 0.847\n",
      "F1 VADER: 0.612\n",
      "F1 Word: 0.565\n",
      "F1 Sentence: 0.599\n"
     ]
    }
   ],
   "source": [
    "# Train and test with Stratified K Fold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores_clf = []\n",
    "scores_vader = []\n",
    "scores_sentence = []\n",
    "scores_word = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(corpus_wo_objective, ref)):\n",
    "    x_train, x_test = [corpus_wo_objective[indx] for indx in train_index], [corpus_wo_objective[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    hyp = clf.predict(test_features)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    hyp_vader = [polarity_doc_level(doc, analyzer) for doc in x_test]\n",
    "    scores_vader.append(f1_score(y_test, hyp_vader, average='macro'))\n",
    "    \n",
    "    hyp_word = [polarity_word_level(doc, analyzer) for doc in test_x_split]\n",
    "    scores_word.append(f1_score(y_test, hyp_word, average='macro'))\n",
    "    \n",
    "    hyp_sentence = [polarity_sentence_level(doc, analyzer) for doc in test_x_split]\n",
    "    scores_sentence.append(f1_score(y_test, hyp_sentence, average='macro'))\n",
    "    \n",
    "    \n",
    "print('F1 classifier:', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('F1 VADER:',  round(sum(scores_vader)/len(scores_vader), 3))\n",
    "print('F1 Word:',  round(sum(scores_word)/len(scores_word), 3))\n",
    "print('F1 Sentence:',  round(sum(scores_sentence)/len(scores_sentence), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
