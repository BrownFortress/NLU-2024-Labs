{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Statistical Language Modeling with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "- Load Shakespeare's Hamlet from Gutenberg corpus\n",
    "    - lowercase it\n",
    "\n",
    "- Extract padded unigrams and bigrams\n",
    "\n",
    "- Using NgramCounter\n",
    "    - get total number of ngrams\n",
    "    - get count of unigram `the`\n",
    "    - get count of bigram `of the`\n",
    "    \n",
    "|                     | Count  | \n",
    "|---------------------|---------|\n",
    "| Ngrams      | 84038     | \n",
    "| Unigram *the* | 993|\n",
    "| Bigram *of the*     |59 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, NgramCounter\n",
    "\n",
    "hamlet = gutenberg.sents('shakespeare-hamlet.txt')\n",
    "\n",
    "print(len(hamlet))\n",
    "print(hamlet[0])\n",
    "# lowercasing use .lower()\n",
    "hamlet_lowercase = [[w.lower() for w in sent] for sent in hamlet]\n",
    "print(hamlet_lowercase[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, hamlet_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "counter = NgramCounter(padded_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter.N())\n",
    "print(counter['the'])\n",
    "print(counter[['of']]['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "- lookup in vocabulary\n",
    "    - \"trento is the capital city of trentino\"\n",
    "- update vocabulary with \"trento is the capital city of trentino\"\n",
    "    - do the lookup again to see the effect\n",
    "- experiment with changing the cut-off value from `1` to `10`\n",
    "    - do the lookup again to see the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "\n",
    "hamlet_words = gutenberg.words('shakespeare-hamlet.txt')\n",
    "\n",
    "# lowercase\n",
    "hamlet_words = [w.lower() for w in hamlet_words]\n",
    "\n",
    "sentence = \"trento is the capital city of trentino\".split()\n",
    "\n",
    "# Cut-off 0   \n",
    "vocab = Vocabulary(hamlet_words)\n",
    "print(list(vocab.lookup(sentence)))\n",
    "vocab.update(sentence)\n",
    "print(list(vocab.lookup(sentence)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut-off 1\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=1)\n",
    "print(list(vocab.lookup(sentence)))\n",
    "vocab.update(sentence)\n",
    "print(list(vocab.lookup(sentence)))\n",
    "\n",
    "# Cut-off 10\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=10)\n",
    "print(list(vocab.lookup(sentence)))\n",
    "vocab.update(sentence)\n",
    "print(list(vocab.lookup(sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "Implement a function to compute score of a sequence (i.e. Chain Rule)\n",
    "\n",
    "- arguments:\n",
    "    - Language Model\n",
    "    - List of Tokens\n",
    "\n",
    "- functionality\n",
    "    - extracts ngrams w.r.t. LM order (`lm.order`)\n",
    "    - scores each ngram w.r.t. LM (`lm.score` or `lm.logscore`)\n",
    "        - mind that `score` takes care of OOV by conterting to `<UNK>` already\n",
    "    - computes the overal score using chain rule\n",
    "        - mind the difference between `score` and `logscore`\n",
    "\n",
    "- compute the scores of the sentences below\n",
    "    - compute padded and unpadded sequence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_sents = [\"the king is dead\", \"the tzar is dead\", 'the tragedie of hamlet is good']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, everygrams\n",
    "def chain_rule(lm, sentence, log=True, pad=True):\n",
    "    highest_ngram = lm.order\n",
    "    tokens = sentence.split()\n",
    "    if pad:\n",
    "        ngrams, _ = padded_everygram_pipeline(highest_ngram, [tokens])\n",
    "        ngrams = chain.from_iterable(ngrams) # Flat the sequence\n",
    "    else:\n",
    "        ngrams = everygrams(tokens, max_len=highest_ngram)\n",
    "\n",
    "        \n",
    "    if log:\n",
    "        total_score = 0\n",
    "    else:\n",
    "        total_score = 1\n",
    "        \n",
    "    for x in ngrams:\n",
    "        if len(x) == highest_ngram:\n",
    "            if log:\n",
    "                w_t = x[-1]\n",
    "                # In python you can get a split of a tuple or array as array[from:to] \"to\" is excluded\n",
    "                context = x[0:-1] # or x[:-1]\n",
    "                score = lm.logscore(w_t, context)\n",
    "                total_score += score # Add or multiply score to total_score ?\n",
    "            else:\n",
    "                w_t = x[-1]\n",
    "                context = x[0:-1]\n",
    "                score = lm.score(w_t, context)\n",
    "                total_score *= score\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "for sent in test_sents:\n",
    "    print(sent, chain_rule(\"Add your model\", sent, log=True, pad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 4\n",
    "Compute entropy and perplexity of the `MLE` models  on the bigrams of the test sentences below, treating them as a test set.\n",
    "\n",
    "- experiment with the two test sets\n",
    "- experiment with OOVs (with vs without)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_sents1 = [\"the king is dead\", \"the emperor is dead\", \"may the force be with you\"]\n",
    "test_sents2 = [\"the king is dead\", \"welcome to you\", \"how are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline, flatten\n",
    "from nltk.lm import Vocabulary, MLE\n",
    "\n",
    "# Load data\n",
    "hamlet_sents = [[w.lower() for w in sent] for sent in gutenberg.sents('shakespeare-hamlet.txt')]\n",
    "hamlet_words = flatten(hamlet_sents)\n",
    "# Compute vocab \n",
    "lex = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "# Handeling OOV\n",
    "hamlet_oov_sents = [list(lex.lookup(sent)) for sent in hamlet_sents]\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(2, hamlet_oov_sents)\n",
    "# Train the model \n",
    "lm_oov = MLE(2)\n",
    "lm_oov.fit(padded_ngrams_oov, flat_text_oov)\n",
    "# Compute PPL and entropu with OOV on test 1\n",
    "test_set = test_sents2\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "ngrams = chain.from_iterable(ngrams)\n",
    "ppl =  lm_oov.perplexity([x for x in ngrams   if len(x) == lm_oov.order])\n",
    "print('PPL:', ppl)\n",
    "# Generators are one-use only!\n",
    "ngrams, flat_text = padded_everygram_pipeline(lm_oov.order, [lex.lookup(sent.split()) for sent in test_set])\n",
    "ngrams = chain.from_iterable(ngrams)\n",
    "cross_entropy = lm_oov.entropy([x for x in ngrams  if len(x) == lm_oov.order])\n",
    "print('Cross Entropy :', cross_entropy)\n",
    "print('\\t PPL:', pow(2, cross_entropy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PP: how it works inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def compute_ppl(model, data):\n",
    "    highest_ngram = model.order\n",
    "    scores = [] \n",
    "    for sentence in data:\n",
    "        ngrams, flat_text = padded_everygram_pipeline(highest_ngram, [sentence.split()])\n",
    "        scores.extend([model.logscore(w[-1], w[:-1]) for gen in ngrams for w in gen if len(w) == highest_ngram])\n",
    "    \n",
    "    return math.pow(2.0, (-1 * np.asarray(scores).mean()))\n",
    "\n",
    "compute_ppl(\"Add your model\", test_sents2)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
